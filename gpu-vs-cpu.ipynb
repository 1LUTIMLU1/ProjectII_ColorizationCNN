{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sys import platform\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tunedColorizer import colorizer\n",
    "from pytorchtool import EarlyStopping\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'darwin':\n",
    "    slash = '/'\n",
    "else: \n",
    "    slash = '\\\\'\n",
    "\n",
    "def load(folder):\n",
    "    files = glob.glob(folder)\n",
    "    data =[]\n",
    "    for f in files:\n",
    "        image = cv2.imread(f)\n",
    "        data.append(image)\n",
    "    return data\n",
    "\n",
    "\n",
    "def LoadLabInOrder(folder):\n",
    "\n",
    "    # find the largest number file to index (do not worry about how these next few lines work)\n",
    "    files = glob.glob(folder + \"*L.jpg\")\n",
    "    for i, f in enumerate(files):\n",
    "        f = f[f.rfind(slash)+1:]\n",
    "        files[i] = f[0:f.rfind('.')-1]\n",
    "    maxFileNum = max([int(f) for f in files])\n",
    "    \n",
    "    # for each file index (e.g. ['0a.jpg', '0b.jpg', '0L.jpg'])\n",
    "    data = []\n",
    "    for i in range(0, maxFileNum):\n",
    "        # grab files in order 'a', 'b', 'L'\n",
    "        files = sorted(glob.glob(folder + str(i) + \"?.jpg\"), key=str.casefold)\n",
    "\n",
    "        # append each file\n",
    "        for f in files:\n",
    "            image = cv2.imread(f)\n",
    "\n",
    "            # only take one channel (all the channels here are the same)\n",
    "            data.append(image)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def group(data, album_length):\n",
    "    #group into chunks of three because of three sets of images in LAB color space\n",
    "    for i in range (0, album_length, 3):\n",
    "        yield data[i:i+3]\n",
    "  \n",
    "def make_numpy_format(image):\n",
    "    image = torch.swapaxes(image, 0, 1)\n",
    "    image = torch.swapaxes(image, 1, 2)\n",
    "    return image\n",
    "\n",
    "def make_tensor(image):\n",
    "    image = np.swapaxes(image, 2,1)\n",
    "    image = np.swapaxes(image, 1,0)\n",
    "    return image\n",
    "\n",
    "\n",
    "class imageDataset(Dataset):\n",
    "    def __init__(self,  l_color_space, ab_color_space,):\n",
    "        #dropping redundant channels\n",
    "        a = (ab_color_space[:, 0])\n",
    "        b = (ab_color_space[:, 1])\n",
    "        l = (l_color_space)\n",
    "        \n",
    "\n",
    "        #it seems that I will have to use permute \n",
    "        #to get from numpy image representation\n",
    "        #to torch tensor image \n",
    "        \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.indices = len(l)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.indices\n",
    "      \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.a[index], self.b[index], self.l[index]    \n",
    "    \n",
    "    \n",
    "def convert_LAB(album):\n",
    "    converted_album = [[] for _ in range( len(album))]\n",
    "    \n",
    "    #code borrowed from https://towardsdatascience.com/computer-vision-101-working-with-color-images-in-python-7b57381a8a54\n",
    "    for index, image in enumerate(album):\n",
    "        #do LAB conversion here\n",
    "        image = np.asarray(image).astype('uint8')\n",
    "        converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        \n",
    "        names = []\n",
    "        channels = []\n",
    "        \n",
    "        for (name, chan) in zip((\"L\", \"a\", \"b\"), cv2.split(converted_image)):\n",
    "            names.append(name)\n",
    "            channels.append(chan)\n",
    "           # cv2.imshow(name, chan)\n",
    "           \n",
    "        converted_album[index] = list(zip(names, channels))\n",
    "              \n",
    "    return converted_album\n",
    "        \n",
    "def saveLAB(album, folder_name):\n",
    "    subfolder_dir = os.path.join(os.getcwd(), folder_name)\n",
    "    \n",
    "    if not os.path.exists(subfolder_dir):\n",
    "        os.mkdir(subfolder_dir)\n",
    "\n",
    "    count = 0\n",
    "    #each entry inside an LAB color space album is a tuple\n",
    "    for tup in album:\n",
    "       # image = image.cpu().detach().numpy()\n",
    "       names, channels = zip(*tup)\n",
    "       \n",
    "       for i in range(len(channels)):\n",
    "           name = names[i]\n",
    "           channel = channels[i]\n",
    "           cv2.imwrite(subfolder_dir + slash + str(count) + name + '.jpg', channel)\n",
    "           \n",
    "      \n",
    "       count +=1\n",
    "\n",
    "home_dir = os.getcwd() \n",
    "\n",
    "#change this parameter depending on which album you want\n",
    "target_album = 'ColorfulLab'\n",
    "album = 'fruit'\n",
    "\n",
    "# if the specified directory does not exist, or if it exists but is empty\n",
    "if not os.path.exists(home_dir + slash + target_album) \\\n",
    "    or (os.path.exists(home_dir + slash + target_album) and not [name for name in os.listdir(\".\" + slash + target_album)]):\n",
    "\n",
    "    # get names of folders for colorful fruit data\n",
    "    foodFolders = [name for name in os.listdir(\".\" + slash + target_album)]\n",
    "\n",
    "    # get fruit images\n",
    "    food_images = []\n",
    "    for ff in foodFolders:\n",
    "        food_images.extend(load(home_dir + slash + target_album + slash + ff + slash + '*.jpg'))\n",
    "\n",
    "    # album_length = len(food_images)\n",
    "\n",
    "    for i, val in enumerate(food_images):\n",
    "        food_images[i] = cv2.resize(val, (128, 128))\n",
    "    food_images_lab = convert_LAB(food_images)\n",
    "    saveLAB(food_images_lab, \"ColorfulLab\")\n",
    "\n",
    "batch_size = 32\n",
    "Epochs = 100\n",
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "food_data = LoadLabInOrder(home_dir + slash + target_album + slash)\n",
    "album_length = len(food_data)\n",
    "\n",
    "#group images into sets of 3   \n",
    "food_grouped_data = list(group(food_data, album_length))\n",
    "food_grouped_data = np.asarray(food_grouped_data)\n",
    "    \n",
    "#prepare grouped data for training and test\n",
    "food_train_images, food_test_images = train_test_split(food_grouped_data, test_size = 0.3)\n",
    "food_train_images, food_val_images = train_test_split(food_train_images, test_size = 0.1)\n",
    "\n",
    "#further separate them into X's and Y's where L is the input and AB are the targets (LAB colorspace)\n",
    "#remember the dimensions are Number of grouped images X Index of image\n",
    "#this needs to be flipped\n",
    "\n",
    "food_X_train = food_train_images[:, 2, :, :, 0]\n",
    "food_y_train = food_train_images[:, 0:2, :, :, 0]\n",
    "\n",
    "food_X_test = food_test_images[:, 2, :, :, 0]\n",
    "food_y_test = food_test_images[:, 0:2, :, :, 0]\n",
    "\n",
    "food_X_val = food_test_images[:, 2, :, :, 0]\n",
    "food_y_val = food_test_images[:, 0:2, :, :, 0]\n",
    "\n",
    "\n",
    "#prepare datasets for images\n",
    "food_train_dataset = imageDataset(food_X_train, food_y_train)\n",
    "food_test_dataset = imageDataset(food_X_test, food_y_test)\n",
    "food_val_dataset = imageDataset(food_X_val, food_y_val)\n",
    "\n",
    "# prepare dataloaders for batch training\n",
    "food_train_loader = DataLoader(dataset = food_train_dataset, batch_size = batch_size, shuffle=True)\n",
    "food_test_loader = DataLoader(dataset = food_test_dataset,  batch_size = batch_size, shuffle=True)\n",
    "food_val_loader = DataLoader(dataset = food_val_dataset,  batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Of Images Tested = 224\n",
      "Testing MSE Loss = tensor(124.8859)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' #'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load model\n",
    "path = \"chkpt_fruit\\color_model_40.pt\"\n",
    "bestModel = colorizer().to(device)\n",
    "bestModel.load_state_dict(torch.load(path))\n",
    "bestModel.eval()\n",
    "\n",
    "# calculate and print the loss on the test images\n",
    "running_test_loss = 0.0\n",
    "result = []\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    bestModel.eval()\n",
    "    \n",
    "    # for each test image\n",
    "    for i, data in enumerate(food_test_loader):\n",
    "\n",
    "        # get the inputs and outputs\n",
    "        test_l = torch.unsqueeze(data[2], 1).to(device)\n",
    "        start = time.time()\n",
    "        test_outputs = bestModel(test_l)\n",
    "        end = time.time()\n",
    "        times.extend([end-start])\n",
    "        test_labels = torch.stack((data[0], data[1]), 1).float().to(device)\n",
    "        test_loss = criterion(test_outputs, torch.flatten(test_labels, 0, 1))\n",
    "        running_test_loss += test_loss\n",
    "\n",
    "print(\"\\nNumber Of Images Tested =\", len(food_test_loader)*batch_size)\n",
    "print(\"Testing MSE Loss =\", (running_test_loss/len(food_test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13945582934788295  secs\n"
     ]
    }
   ],
   "source": [
    "mean_times = np.mean(times)\n",
    "print(mean_times, \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12347ee61ea2ffef8bdd924564b209dba478b26ce284a1f8bfda94c61015ebbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
