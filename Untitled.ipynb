{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct 20 12:24:56 2022\n",
    "\n",
    "@author: Timothy Lu\n",
    "\"\"\"\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# For utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import normalize\n",
    "from sys import platform\n",
    "from itertools import combinations\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5a7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CNN resource: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html\n",
    "if platform == 'darwin':\n",
    "    slash = '/'\n",
    "else: \n",
    "    slash = '\\\\'\n",
    " \n",
    "    \n",
    "def load(folder):\n",
    "    files = glob.glob(folder)\n",
    "    data =[]\n",
    "    for f in files:\n",
    "        image = cv2.imread(f)\n",
    "        data.append(image)\n",
    "    return data\n",
    "\n",
    "def group(data, album_length):\n",
    "    #group into chunks of three because of three sets of images in LAB color space\n",
    "    for i in range (0, album_length, 3):\n",
    "        yield image_data[i:i+3]\n",
    "    \n",
    "\n",
    "\n",
    "class imageDataset(Dataset):\n",
    "    def __init__(self,  l_color_space, ab_color_space,):\n",
    "        a = (ab_color_space[:, 0, :, :, :])\n",
    "        b = (ab_color_space[:, 1, :, :, :])\n",
    "        l = (l_color_space)\n",
    "        \n",
    "        #it seems that I will have to use permute \n",
    "        #to get from numpy image representation\n",
    "        #to torch tensor image \n",
    "        \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.indices = len(l)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.indices\n",
    "      \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.a[index], self.b[index], self.l[index]\n",
    "      \n",
    "class chrominance_reg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(chrominance_reg, self).__init__()\n",
    "        \n",
    "        #128x128\n",
    "        self.mod1 = nn.Sequential(\n",
    "             nn.Conv2d(3, 6, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #64x64\n",
    "        self.mod2 = nn.Sequential(\n",
    "             nn.Conv2d(6, 12, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #32x32\n",
    "        self.mod3 = nn.Sequential(\n",
    "             nn.Conv2d(12, 24, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #16x16\n",
    "        self.mod4 = nn.Sequential(\n",
    "             nn.Conv2d(24, 48, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #8x8\n",
    "        self.mod5 = nn.Sequential(\n",
    "             nn.Conv2d(48, 96, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #4x4\n",
    "        self.mod6 = nn.Sequential(\n",
    "             nn.Conv2d(96, 192, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #2x2\n",
    "        self.mod7 = nn.Sequential(\n",
    "          \n",
    "             nn.Conv2d(192, 384, kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.Flatten(),\n",
    "             nn.Linear(1,2)\n",
    "             )\n",
    "           \n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Normalize input first\n",
    "        make_Tensor = T.ToTensor()\n",
    "     #   transform_pil = T.ToPILImage()\n",
    "       \n",
    "        out = make_Tensor(x)\n",
    "        #out = transform_pil(x)\n",
    "        out = normalize(out)\n",
    "        out = self.mod1(out)\n",
    "        out = self.mod2(out)\n",
    "        out = self.mod3(out)\n",
    "        out = self.mod4(out)\n",
    "        out = self.mod5(out)\n",
    "        out = self.mod6(out)\n",
    "        out = self.mod7(out)\n",
    "        out = torch.mean(out,0, True)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26eccd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.getcwd() \n",
    "#change this parameter depending on which album you want\n",
    "target_album = 'LAB_TEST_FACES'\n",
    "\n",
    "\n",
    "\n",
    "image_data = load(home_dir + slash + target_album + slash + '*.jpg')\n",
    "album_length = len(image_data)\n",
    "\n",
    "#group images into sets of 3   \n",
    "grouped_data = list(group(image_data, album_length))\n",
    "grouped_data = np.asarray(grouped_data)\n",
    "\n",
    "#prepare grouped data for training and test\n",
    "train_images, test_images = train_test_split(grouped_data, test_size = 0.3)\n",
    "\n",
    "#further separate them into X's and Y's where L is the input and AB are the targets (LAB colorspace)\n",
    "#remember the dimensions are Number of grouped images X Index of image\n",
    "#this needs to be flipped\n",
    "\n",
    "X_train = train_images[:, 2, :, :, :]\n",
    "y_train = train_images[:, 0:2, :, :, :]\n",
    "\n",
    "\n",
    "X_test = test_images[:, 2, :, :, :]\n",
    "y_test = test_images[:, 0:2, :, :, :]\n",
    "\n",
    "\n",
    "\n",
    "#prepare datasets for images\n",
    "train_dataset = imageDataset(X_train, y_train)\n",
    "test_dataset = imageDataset(X_test, y_test)\n",
    "\n",
    "#prepare dataloaders for batch training\n",
    "#create datasets\n",
    "batch_size = 10\n",
    "Epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size)\n",
    "\n",
    "#call the regressor on one set of images in the X_train dataset\n",
    "regressor = chrominance_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9051b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[132., 132., 132.],\n",
       "          [131., 131., 131.],\n",
       "          [130., 130., 130.],\n",
       "          ...,\n",
       "          [135., 135., 135.],\n",
       "          [136., 136., 136.],\n",
       "          [137., 137., 137.]],\n",
       "\n",
       "         [[133., 133., 133.],\n",
       "          [133., 133., 133.],\n",
       "          [132., 132., 132.],\n",
       "          ...,\n",
       "          [135., 135., 135.],\n",
       "          [136., 136., 136.],\n",
       "          [137., 137., 137.]],\n",
       "\n",
       "         [[133., 133., 133.],\n",
       "          [134., 134., 134.],\n",
       "          [134., 134., 134.],\n",
       "          ...,\n",
       "          [135., 135., 135.],\n",
       "          [136., 136., 136.],\n",
       "          [137., 137., 137.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[139., 139., 139.],\n",
       "          [139., 139., 139.],\n",
       "          [140., 140., 140.],\n",
       "          ...,\n",
       "          [137., 137., 137.],\n",
       "          [137., 137., 137.],\n",
       "          [137., 137., 137.]],\n",
       "\n",
       "         [[139., 139., 139.],\n",
       "          [139., 139., 139.],\n",
       "          [139., 139., 139.],\n",
       "          ...,\n",
       "          [137., 137., 137.],\n",
       "          [137., 137., 137.],\n",
       "          [137., 137., 137.]],\n",
       "\n",
       "         [[139., 139., 139.],\n",
       "          [139., 139., 139.],\n",
       "          [138., 138., 138.],\n",
       "          ...,\n",
       "          [137., 137., 137.],\n",
       "          [137., 137., 137.],\n",
       "          [137., 137., 137.]]],\n",
       "\n",
       "\n",
       "        [[[135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          ...,\n",
       "          [127., 127., 127.],\n",
       "          [128., 128., 128.],\n",
       "          [129., 129., 129.]],\n",
       "\n",
       "         [[135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          ...,\n",
       "          [127., 127., 127.],\n",
       "          [128., 128., 128.],\n",
       "          [129., 129., 129.]],\n",
       "\n",
       "         [[135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          ...,\n",
       "          [126., 126., 126.],\n",
       "          [127., 127., 127.],\n",
       "          [128., 128., 128.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[147., 147., 147.],\n",
       "          [147., 147., 147.],\n",
       "          [146., 146., 146.],\n",
       "          ...,\n",
       "          [149., 149., 149.],\n",
       "          [150., 150., 150.],\n",
       "          [151., 151., 151.]],\n",
       "\n",
       "         [[148., 148., 148.],\n",
       "          [147., 147., 147.],\n",
       "          [146., 146., 146.],\n",
       "          ...,\n",
       "          [149., 149., 149.],\n",
       "          [150., 150., 150.],\n",
       "          [152., 152., 152.]],\n",
       "\n",
       "         [[148., 148., 148.],\n",
       "          [147., 147., 147.],\n",
       "          [146., 146., 146.],\n",
       "          ...,\n",
       "          [150., 150., 150.],\n",
       "          [151., 151., 151.],\n",
       "          [152., 152., 152.]]],\n",
       "\n",
       "\n",
       "        [[[134., 134., 134.],\n",
       "          [134., 134., 134.],\n",
       "          [134., 134., 134.],\n",
       "          ...,\n",
       "          [138., 138., 138.],\n",
       "          [139., 139., 139.],\n",
       "          [138., 138., 138.]],\n",
       "\n",
       "         [[135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          [134., 134., 134.],\n",
       "          ...,\n",
       "          [139., 139., 139.],\n",
       "          [139., 139., 139.],\n",
       "          [139., 139., 139.]],\n",
       "\n",
       "         [[136., 136., 136.],\n",
       "          [135., 135., 135.],\n",
       "          [135., 135., 135.],\n",
       "          ...,\n",
       "          [140., 140., 140.],\n",
       "          [140., 140., 140.],\n",
       "          [139., 139., 139.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[148., 148., 148.],\n",
       "          [147., 147., 147.],\n",
       "          [146., 146., 146.],\n",
       "          ...,\n",
       "          [109., 109., 109.],\n",
       "          [111., 111., 111.],\n",
       "          [114., 114., 114.]],\n",
       "\n",
       "         [[147., 147., 147.],\n",
       "          [147., 147., 147.],\n",
       "          [146., 146., 146.],\n",
       "          ...,\n",
       "          [109., 109., 109.],\n",
       "          [111., 111., 111.],\n",
       "          [112., 112., 112.]],\n",
       "\n",
       "         [[147., 147., 147.],\n",
       "          [146., 146., 146.],\n",
       "          [145., 145., 145.],\n",
       "          ...,\n",
       "          [109., 109., 109.],\n",
       "          [111., 111., 111.],\n",
       "          [111., 111., 111.]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91828be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[132., 131., 130.,  ..., 135., 136., 137.],\n",
       "        [133., 133., 132.,  ..., 135., 136., 137.],\n",
       "        [133., 134., 134.,  ..., 135., 136., 137.],\n",
       "        ...,\n",
       "        [139., 139., 140.,  ..., 137., 137., 137.],\n",
       "        [139., 139., 139.,  ..., 137., 137., 137.],\n",
       "        [139., 139., 138.,  ..., 137., 137., 137.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(train_dataset[0]).permute(0, 3, 1, 2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dc1c256",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [6, 3, 3, 3], but got 3-dimensional input of size [3, 128, 128] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6q/lnztzmln25l88yjxl7j0pxth0000gn/T/ipykernel_22338/2406033874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#run forward pass on one grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_grayscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchrome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_grayscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6q/lnztzmln25l88yjxl7j0pxth0000gn/T/ipykernel_22338/2011852687.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m#out = transform_pil(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [6, 3, 3, 3], but got 3-dimensional input of size [3, 128, 128] instead"
     ]
    }
   ],
   "source": [
    "#run forward pass on one grayscale image\n",
    "sample_a, sample_b, sample_grayscale = train_dataset[0]\n",
    "chrome = regressor(sample_grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38400f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d381ba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [6, 3, 3, 3], but got 3-dimensional input of size [3, 128, 128] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6q/lnztzmln25l88yjxl7j0pxth0000gn/T/ipykernel_22338/1277536508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#run forward pass on one grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0msample_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_grayscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mchrome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_grayscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6q/lnztzmln25l88yjxl7j0pxth0000gn/T/ipykernel_22338/2011852687.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m#out = transform_pil(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [6, 3, 3, 3], but got 3-dimensional input of size [3, 128, 128] instead"
     ]
    }
   ],
   "source": [
    "#set paths \n",
    "#resource:\n",
    "#https://www.kaggle.com/code/anirbanmalick/image-colorization-pytorch-conv-autoencoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#run color regressor\n",
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(regressor.parameters(), lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(Epochs):  # loop over the dataset multiple times\n",
    "    regressor.train()\n",
    "   \n",
    "    running_loss = 0.0\n",
    "    #I want batch to be of length 10 not 3 why?\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        batch_a = batch[0]\n",
    "        batch_b = batch[1]\n",
    "        batch_l = batch[2]\n",
    "    \n",
    "        \n",
    "        #each batch is ten images so loop through all the images per batch\n",
    "        \n",
    "        for index, images in enumerate(batch):\n",
    "            # get the inputs; data is a list of tensors [chrominance_a_tensor, chrominance_b_tensor, grayscale_l_tensor]\n",
    "          #different images!\n",
    "            label_a = normalize(batch_a[index].float())\n",
    "            label_b = normalize(batch_b[index].float())\n",
    "            input_l = normalize(batch_l[index].float())\n",
    "            \n",
    "            #need to get the mean of labels across all dimensions\n",
    "            mean_a = torch.mean(label_a, dim = [0, 1, 2])\n",
    "            mean_b = torch.mean(label_b, dim = [0, 1, 2])\n",
    "            \n",
    "            labels = torch.tensor((mean_a, mean_b))\n",
    "            #add new axis to make it consistent with dimension of regressor output tensor\n",
    "            labels = torch.unsqueeze(labels, 0)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = regressor(np.asarray(input_l))\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "                  \n",
    "        \n",
    "    train_loss.append(loss)\n",
    "    print('Epoch {} of {}, Train Loss: {:.3f}'.format( epoch+1, Epochs, loss))\n",
    "          \n",
    "      \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36879358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a44f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b981696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67def1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd1864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
