{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f26c2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sys import platform\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tunedColorizer import colorizer\n",
    "# from cnnColorizer import colorizer, trainModel\n",
    "from pytorchtool import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2d9b3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class colorer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(colorer, self).__init__()\n",
    "        \n",
    "#         outSize = [8, 16, 32, 128, 256, 128, 32, 16, 8]\n",
    "        outSize = [128, 256, 256, 200, 128, 64, 32, 16, 4] \n",
    "#         outSize = [2, 4, 8, 16, 32, 16, 8, 4, 2]\n",
    "#         outSize = [4, 16, 32, 64, 128, 200, 256, 256, 128]\n",
    "    \n",
    "        #128x128\n",
    "        self.downsamp1 = nn.Sequential(\n",
    "             nn.Conv2d(1, outSize[0], kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(outSize[0]),\n",
    "            # nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #64x64\n",
    "        self.downsamp2 = nn.Sequential(\n",
    "             nn.Conv2d(outSize[0], outSize[1], kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(outSize[1]),\n",
    "           #  nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #32x32\n",
    "        self.downsamp3 = nn.Sequential(\n",
    "             nn.Conv2d(outSize[1], outSize[2], kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(outSize[2]),\n",
    "          #   nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #16x16\n",
    "        self.downsamp4 = nn.Sequential(\n",
    "             nn.Conv2d(outSize[2], outSize[3], kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(outSize[3]),\n",
    "          #   nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        #8x8\n",
    "        self.downsamp5 = nn.Sequential(\n",
    "             nn.Conv2d(outSize[3], outSize[4], kernel_size = 3, stride = 2, padding = 1),\n",
    "             nn.ReLU(),\n",
    "             nn.BatchNorm2d(outSize[4]),\n",
    "          #   nn.AvgPool2d(kernel_size = (1,1), stride = 1)\n",
    "             )\n",
    "        \n",
    "        #begin upsampling here\n",
    "        #using convolution transpose\n",
    "        #8x8\n",
    "        self.upsamp1 = nn.Sequential(        \n",
    "             nn.ConvTranspose2d(outSize[4], outSize[5], kernel_size = 2, stride = 2),\n",
    "             nn.BatchNorm2d(outSize[5]),\n",
    "             )\n",
    "        \n",
    "        #16x16\n",
    "        self.upsamp2 = nn.Sequential(         \n",
    "             nn.ConvTranspose2d(outSize[5], outSize[6], kernel_size = 2, stride = 2),\n",
    "             nn.BatchNorm2d(outSize[6]),\n",
    "             )\n",
    "        \n",
    "        #32x32\n",
    "        self.upsamp3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(outSize[6], outSize[7], kernel_size = 2, stride = 2),    \n",
    "            nn.BatchNorm2d(outSize[7]),\n",
    "            )\n",
    "        \n",
    "        #64x64\n",
    "        self.upsamp4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(outSize[7], outSize[8], kernel_size = 2, stride = 2),    \n",
    "            nn.BatchNorm2d(outSize[8]),\n",
    "            )\n",
    "        \n",
    "         #128x128\n",
    "         #keep at 2 channels\n",
    "        self.upsamp5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(outSize[8], 2, kernel_size = 2, stride = 2),         \n",
    "            )\n",
    "           \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.float()\n",
    "        out = self.downsamp1(x)\n",
    "        out = self.downsamp2(out)\n",
    "        out = self.downsamp3(out)\n",
    "        out = self.downsamp4(out)\n",
    "        out = self.downsamp5(out)\n",
    "        out = self.upsamp1(out)\n",
    "        out = self.upsamp2(out)\n",
    "        out = self.upsamp3(out)\n",
    "        out = self.upsamp4(out)\n",
    "        out = self.upsamp5(out)\n",
    "\n",
    "        #collapse dimension 1 along dimension 0 using flatten\n",
    "        out = torch.flatten(out, 0, 1)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d5f19",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "45aa4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(folder):\n",
    "    files = glob.glob(folder)\n",
    "    data =[]\n",
    "    for f in files:\n",
    "        image = cv2.imread(f)\n",
    "        data.append(image)\n",
    "    return data\n",
    "\n",
    "\n",
    "def LoadLabInOrder(folder):\n",
    "\n",
    "    # find the largest number file to index (do not worry about how these next few lines work)\n",
    "    files = glob.glob(folder + \"*L.jpg\")\n",
    "    for i, f in enumerate(files):\n",
    "        f = f[f.rfind('/')+1:]\n",
    "        files[i] = f[0:f.rfind('.')-1]\n",
    "    maxFileNum = max([int(f) for f in files])\n",
    "    \n",
    "    # for each file index (e.g. ['0a.jpg', '0b.jpg', '0L.jpg'])\n",
    "    data = []\n",
    "    for i in range(0, maxFileNum):\n",
    "        # grab files in order 'a', 'b', 'L'\n",
    "        files = sorted(glob.glob(folder + str(i) + \"?.jpg\"), key=str.casefold)\n",
    "\n",
    "        # append each file\n",
    "        for f in files:\n",
    "            image = cv2.imread(f)\n",
    "\n",
    "            # only take one channel (all the channels here are the same)\n",
    "            data.append(image)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def group(data, album_length):\n",
    "    #group into chunks of three because of three sets of images in LAB color space\n",
    "    for i in range (0, album_length, 3):\n",
    "        yield data[i:i+3]\n",
    "  \n",
    "def make_numpy_format(image):\n",
    "    image = torch.swapaxes(image, 0, 1)\n",
    "    image = torch.swapaxes(image, 1, 2)\n",
    "    return image\n",
    "\n",
    "def make_tensor(image):\n",
    "    image = np.swapaxes(image, 2,1)\n",
    "    image = np.swapaxes(image, 1,0)\n",
    "    return image\n",
    "\n",
    "\n",
    "class imageDataset(Dataset):\n",
    "    def __init__(self,  l_color_space, ab_color_space,):\n",
    "        #dropping redundant channels\n",
    "        a = (ab_color_space[:, 0])\n",
    "        b = (ab_color_space[:, 1])\n",
    "        l = (l_color_space)\n",
    "        \n",
    "\n",
    "        #it seems that I will have to use permute \n",
    "        #to get from numpy image representation\n",
    "        #to torch tensor image \n",
    "        \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.indices = len(l)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.indices\n",
    "      \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.a[index], self.b[index], self.l[index]    \n",
    "    \n",
    "    \n",
    "def convert_LAB(album):\n",
    "    converted_album = [[] for _ in range( len(album))]\n",
    "    \n",
    "    #code borrowed from https://towardsdatascience.com/computer-vision-101-working-with-color-images-in-python-7b57381a8a54\n",
    "    for index, image in enumerate(album):\n",
    "        #do LAB conversion here\n",
    "        image = np.asarray(image).astype('uint8')\n",
    "        converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        \n",
    "        names = []\n",
    "        channels = []\n",
    "        \n",
    "        for (name, chan) in zip((\"L\", \"a\", \"b\"), cv2.split(converted_image)):\n",
    "            names.append(name)\n",
    "            channels.append(chan)\n",
    "           # cv2.imshow(name, chan)\n",
    "           \n",
    "        converted_album[index] = list(zip(names, channels))\n",
    "              \n",
    "    return converted_album\n",
    "        \n",
    "def saveLAB(album, folder_name):\n",
    "    subfolder_dir = os.path.join(os.getcwd(), folder_name)\n",
    "    \n",
    "    if not os.path.exists(subfolder_dir):\n",
    "        os.mkdir(subfolder_dir)\n",
    "\n",
    "    count = 0\n",
    "    #each entry inside an LAB color space album is a tuple\n",
    "    for tup in album:\n",
    "       # image = image.cpu().detach().numpy()\n",
    "       names, channels = zip(*tup)\n",
    "       \n",
    "       for i in range(len(channels)):\n",
    "           name = names[i]\n",
    "           channel = channels[i]\n",
    "           cv2.imwrite(subfolder_dir + slash + str(count) + name + '.jpg', channel)\n",
    "           \n",
    "      \n",
    "       count +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d7bb2",
   "metadata": {},
   "source": [
    "### Get Original Images, Convert to Lab, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "08b6f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'darwin':\n",
    "    slash = '/'\n",
    "else: \n",
    "    slash = '\\\\'\n",
    "\n",
    "    \n",
    "home_dir = os.getcwd() \n",
    "\n",
    "#change this parameter depending on which album you want\n",
    "target_album = 'ColorfulLab'\n",
    "if target_album == 'LAB_TEST_FACES':\n",
    "    album = 'faces'\n",
    "else:\n",
    "    album = 'ColorfulLab'\n",
    "\n",
    "# if the specified directory does not exist, or if it exists but is empty\n",
    "if not os.path.exists(home_dir + slash + target_album) \\\n",
    "    or (os.path.exists(home_dir + slash + target_album) and not [name for name in os.listdir(\".\" + slash + target_album)]):\n",
    "\n",
    "    # get names of folders for colorful fruit data\n",
    "    foodFolders = [name for name in os.listdir(\".\" + slash + target_album)]\n",
    "\n",
    "    # get fruit images\n",
    "    food_images = []\n",
    "    for ff in foodFolders:\n",
    "        food_images.extend(load(home_dir + slash + target_album + slash + ff + slash + '*.jpg'))\n",
    "\n",
    "    # album_length = len(food_images)\n",
    "\n",
    "    for i, val in enumerate(food_images):\n",
    "        food_images[i] = cv2.resize(val, (128, 128))\n",
    "    food_images_lab = convert_LAB(food_images)\n",
    "    saveLAB(food_images_lab, \"ColorfulLab\")\n",
    "\n",
    "    # plot rgb image\n",
    "#     plt.imshow(cv2.cvtColor(food_images[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14621dea",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5fc5e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "Epochs = 100\n",
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "home_dir = os.getcwd() \n",
    "#change this parameter depending on which album you want\n",
    "target_album = 'ColorfulLab'\n",
    "if target_album == 'LAB_TEST_FACES':\n",
    "    album = 'faces'\n",
    "else:\n",
    "    album = 'ColorfulLab'\n",
    "\n",
    "\n",
    "food_data = LoadLabInOrder(home_dir + slash + target_album + slash)\n",
    "album_length = len(food_data)\n",
    "\n",
    "#group images into sets of 3   \n",
    "food_grouped_data = list(group(food_data, album_length))\n",
    "food_grouped_data = np.asarray(food_grouped_data)\n",
    "    \n",
    "#prepare grouped data for training and test\n",
    "food_train_images, food_test_images = train_test_split(food_grouped_data, test_size = 0.3)\n",
    "food_train_images, food_val_images = train_test_split(food_train_images, test_size = 0.1)\n",
    "\n",
    "#further separate them into X's and Y's where L is the input and AB are the targets (LAB colorspace)\n",
    "#remember the dimensions are Number of grouped images X Index of image\n",
    "#this needs to be flipped\n",
    "\n",
    "food_X_train = food_train_images[:, 2, :, :, 0]#.astype(dtype=object)\n",
    "food_y_train = food_train_images[:, 0:2, :, :, 0]#.astype(dtype=object)\n",
    "\n",
    "food_X_test = food_test_images[:, 2, :, :, 0]#.astype(dtype=object)\n",
    "food_y_test = food_test_images[:, 0:2, :, :, 0]#.astype(dtype=object)\n",
    "\n",
    "food_X_val = food_test_images[:, 2, :, :, 0]#.astype(dtype=object)\n",
    "food_y_val = food_test_images[:, 0:2, :, :, 0]#.astype(dtype=object)\n",
    "\n",
    "\n",
    "#prepare datasets for images\n",
    "food_train_dataset = imageDataset(food_X_train, food_y_train)\n",
    "food_test_dataset = imageDataset(food_X_test, food_y_test)\n",
    "food_val_dataset = imageDataset(food_X_val, food_y_val)\n",
    "\n",
    "# prepare dataloaders for batch training\n",
    "food_train_loader = torch.utils.data.DataLoader(dataset = food_train_dataset, batch_size = batch_size, shuffle=True)\n",
    "food_test_loader = torch.utils.data.DataLoader(dataset = food_test_dataset,  batch_size = batch_size, shuffle=True)\n",
    "food_val_loader = torch.utils.data.DataLoader(dataset = food_val_dataset,  batch_size = batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b73b1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     for i, val in enumerate(batch):\n",
    "#         print(val[0].shape)\n",
    "#         print(val[1].shape)\n",
    "#         print(val[2].shape)\n",
    "            \n",
    "#     data = [item[0] for item in batch]\n",
    "#     target1 = [item[1] for item in batch]\n",
    "#     target2 = [item[2] for item in batch]\n",
    "#     target = torch.Tensor([target1, target2])\n",
    "#     return [data, target]\n",
    "\n",
    "\n",
    "# # #prepare dataloaders for batch training\n",
    "# food_train_loader = torch.utils.data.DataLoader(dataset = food_train_dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n",
    "# food_test_loader = torch.utils.data.DataLoader(dataset = food_test_dataset,  batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n",
    "# food_val_loader = torch.utils.data.DataLoader(dataset = food_val_dataset,  batch_size = batch_size, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "30dbf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(color, trainLoader, valLoader, optimizer, epochs, album, album_num):\n",
    "\n",
    "    patience = 20\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    if not os.path.exists('.' + slash + f'chkpt_{album}' + slash + f'try{album_num}'):\n",
    "        os.mkdir('.' + slash + f'chkpt_{album}' + slash + f'try{album_num}')\n",
    "    \n",
    "    #training loop: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "    #loss_values = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    val_ticker = 0\n",
    "    last_loss = 20000\n",
    "\n",
    "    # rows, cols = (2, Epochs)\n",
    "    # stored_images = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        color.train()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        #I want batch to be of length 10 not 3 why?\n",
    "        for i, img in enumerate(trainLoader):\n",
    "            \n",
    "            a = img[0] # i changed these for clarity and less typing i didn't want to type batch everytime -hmk\n",
    "            b = img[1]\n",
    "            l = img[2]\n",
    "            \n",
    "            #each batch is ten images so loop through all the images per batch\n",
    "            # no!!!! this defeats the point of batches if you loop through each image you've essentially made your batch size 1 -hmk\n",
    "            \n",
    "            # for index, images in enumerate(batch):\n",
    "                # get the inputs; data is a list of tensors [chrominance_a_tensor, chrominance_b_tensor, grayscale_l_tensor]\n",
    "                # different images!\n",
    "        \n",
    "        \n",
    "            #labels = torch.tensor((label_a, label_b))\n",
    "            #might not be necessary to drop duplicates\n",
    "            labels = torch.stack((a, b), 1).float().to(device)\n",
    "            input_l = torch.unsqueeze(l, 1).to(device)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = color((input_l))\n",
    "            # outputs = outputs.view(2, size)\n",
    "                        \n",
    "            #flatten labels along dimension 0\n",
    "            labels = torch.flatten(labels, 0, 1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "                    \n",
    "        train_loss.append(loss)\n",
    "\n",
    "#         if epoch % 10 == 0:\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            color.eval()\n",
    "            for data in valLoader:\n",
    "                val_l = torch.unsqueeze(data[2], 1).to(device)\n",
    "                val_outputs = color(val_l)\n",
    "                val_labels = torch.stack((data[0], data[1]), 1).float().to(device)\n",
    "                val_loss = criterion(val_outputs, torch.flatten(val_labels, 0, 1))\n",
    "                running_val_loss += val_loss\n",
    "\n",
    "        validation_loss.append(running_val_loss)\n",
    "        print(\"\\nNumber Of Images Tested =\", len(valLoader)*batch_size)\n",
    "        print(\"Validation MSE Loss =\", (running_val_loss/len(valLoader)))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            if (running_val_loss/len(valLoader)) - last_loss >= 0.1:\n",
    "                path = f\"./chkpt_{album}/try{album_num}/color_model_{epoch}.pt\"\n",
    "                torch.save(color.state_dict(), path)\n",
    "        last_loss = (running_val_loss/len(valLoader))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # once done with a loop I want to print out the target image \n",
    "            # # and colorized image for comparison    \n",
    "            sample_target = cv2.merge([l[0].detach().numpy(), a[0].detach().numpy(), b[0].detach().numpy()]) \n",
    "            sample_target = cv2.cvtColor(sample_target, cv2.COLOR_LAB2RGB)\n",
    "            plt.figure()\n",
    "            plt.imshow(sample_target)\n",
    "\n",
    "            sample_target = cv2.merge([l[0].cpu().detach().numpy(), a[0].cpu().detach().numpy(), b[0].cpu().detach().numpy()]) \n",
    "            sample_target = cv2.cvtColor(sample_target, cv2.COLOR_LAB2RGB)\n",
    "            #plt.imshow(sample_target)\n",
    "\n",
    "            colorized_a = outputs[0].cpu().detach().numpy().astype(np.uint8)\n",
    "            colorized_b = outputs[1].cpu().detach().numpy().astype(np.uint8)\n",
    "            sample_colorized = cv2.merge([l[0].detach().numpy(), colorized_a, colorized_b])\n",
    "            sample_colorized = cv2.cvtColor(sample_colorized, cv2.COLOR_LAB2RGB)\n",
    "            plt.figure()\n",
    "            plt.imshow(sample_colorized)                   # dont need these anymore bc im just saving the images as pngs instead -hmk\n",
    "            # stored_images[0][epoch] = sample_target\n",
    "            # stored_images[1][epoch] = sample_colorized\n",
    "            cv2.imwrite(f\"./chkpt_{album}/try{album_num}/images/target_image_{epoch}.png\",sample_target)\n",
    "            cv2.imwrite(f\"./chkpt_{album}/try{album_num}/images/output_image_{epoch}.png\",sample_colorized) # -hmk\n",
    "\n",
    "        \n",
    "        # use early stopping to prevent overfitting\n",
    "        early_stopping(running_val_loss/len(valLoader), color)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    color.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    print('Epoch {} of {}, Training MSE Loss: {:.3f}'.format( epoch+1, epochs, running_loss/len(trainLoader)))\n",
    "        \n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "25e824a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cnnColorizer import colorizer\n",
    "# cModel = torch.load('../saved_models/model_architecture_11.pt')\n",
    "path = \"./saved_models/color_architecture_9.pt\"\n",
    "cModel = colorer()\n",
    "cModel.load_state_dict(torch.load(path))\n",
    "\n",
    "cModel.eval()\n",
    "cModel.downsamp1.requires_grad=False\n",
    "cModel.downsamp2.requires_grad=False\n",
    "cModel.downsamp3.requires_grad=False\n",
    "cModel.downsamp4.requires_grad=False\n",
    "cModel.downsamp5.requires_grad=False\n",
    "cModel.upsamp1.requires_grad=False\n",
    "cModel.upsamp2.requires_grad=False\n",
    "cModel.upsamp3.requires_grad=True\n",
    "cModel.upsamp4.requires_grad=True\n",
    "cModel.upsamp5.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a6ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939012f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(412.2680)\n",
      "Validation loss decreased (inf --> 412.267975).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(346.0075)\n",
      "Validation loss decreased (412.267975 --> 346.007507).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(321.8047)\n",
      "Validation loss decreased (346.007507 --> 321.804749).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(304.6548)\n",
      "Validation loss decreased (321.804749 --> 304.654816).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(306.6493)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(321.3636)\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(293.2835)\n",
      "Validation loss decreased (304.654816 --> 293.283478).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(326.9883)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(285.8238)\n",
      "Validation loss decreased (293.283478 --> 285.823761).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(285.3775)\n",
      "Validation loss decreased (285.823761 --> 285.377502).  Saving model ...\n",
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(288.1771)\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    }
   ],
   "source": [
    "#run color regressor\n",
    "attempt='13'\n",
    "lr = 0.004\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.Adam(cModel.parameters(), lr)\n",
    "trainModel(cModel, food_train_loader, food_val_loader, optimizer, 80, 'fruit', attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f5097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_test_loss = 0.0\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    cModel.eval()\n",
    "    for i, data in enumerate(food_test_loader):\n",
    "        test_l = torch.unsqueeze(data[2], 1).to(device)\n",
    "        test_outputs = cModel(test_l)\n",
    "        test_labels = torch.stack((data[0], data[1]), 1).float().to(device)\n",
    "        test_loss = criterion(test_outputs, torch.flatten(test_labels, 0, 1))\n",
    "        running_test_loss += test_loss\n",
    "        \n",
    "        if i == len(food_test_loader)-1:\n",
    "\n",
    "            a = data[0]\n",
    "            b = data[1]\n",
    "            l = data[2]\n",
    "\n",
    "            test_l = torch.unsqueeze(l, 1).to(device)\n",
    "            outputs = cModel(test_l)\n",
    "            test_a = torch.unsqueeze(a, 1).to(device)\n",
    "            test_b = torch.unsqueeze(b, 1).to(device)\n",
    "\n",
    "print(\"\\nNumber Of Images Tested =\", len(food_test_loader)*batch_size)\n",
    "print(\"Testing MSE Loss =\", (running_test_loss/len(food_test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f195b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37926ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, l.shape[0]):\n",
    "    colorized_a = outputs[2*i-2].cpu().detach().numpy().astype(np.uint8)\n",
    "    colorized_b = outputs[2*i-1].cpu().detach().numpy().astype(np.uint8)\n",
    "    colorized_l = l[i-1].detach().numpy()\n",
    "    sample_colorized = cv2.merge([colorized_l, colorized_a, colorized_b])\n",
    "    sample_colorized = cv2.cvtColor(sample_colorized, cv2.COLOR_LAB2RGB)\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_colorized)\n",
    "    plt.title(str(i))\n",
    "    cv2.imwrite(f\"./chkpt_fruit/sample_results/output_image_{i}.png\",sample_colorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3ff3f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thing = next(iter(food_test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1134087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"./saved_models_fruit/fruit_try_{attempt}.pt\"\n",
    "torch.save(cModel.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe7297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a6047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8b607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39c0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try 2 Architecture\n",
    "## uses color_architecture_9 (model 9)\n",
    "# Froze model parameters until 9th but there are 37\n",
    "# cModel.upsamp4 = nn.Sequential(nn.ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                             nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "# cModel.upsamp4.requires_grad = True\n",
    "# cModel.upsamp5 = nn.Sequential(nn.ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                             nn.BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "# cModel.upsamp5.requires_grad = True\n",
    "# cModel.upsamp6 = nn.Sequential(nn.ConvTranspose2d(4, 2, kernel_size=(2, 2), stride=(2, 2)))\n",
    "# cModel.upsamp6.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try3 \n",
    "# Uses normal architecture but allows training for last 2 layers\n",
    "\n",
    "## Try 4 \n",
    "## uses color_architecture_1 (model 1)\n",
    "# Allows training for last 3 layers\n",
    "\n",
    "## Try 6 \n",
    "## uses color_architecture_9 (model 9)\n",
    "# Allows training for upsampling\n",
    "\n",
    "## Try 7\n",
    "## use color_architecture_1 (model 1)\n",
    "# Only allowed training for upsampling\n",
    "\n",
    "# Try 8\n",
    "## use color_architecture_1 (model 1)\n",
    "# only trained for transition points (start, downsamp->upsamp, end)\n",
    "\n",
    "## ^^ All produce okay results\n",
    "\n",
    "# Most Recent used model 9 with last 3 layers trainable\n",
    "# earlier epoch models and images might not be the same b/c accidentally ran early stopping trial over it\n",
    "\n",
    "# Try 11\n",
    "# uses same stuff as before but with early stopping\n",
    "\n",
    "# Try 12\n",
    "# uses model 1 with early stopping\n",
    "# Returned decent results.  Yellowish background with bland color\n",
    "\n",
    "# Try 13\n",
    "# use model 12 with early stopping (seeing if more filters at the end help). 3 layers were trainable\n",
    "# yellow background, did well on yellow stuff; did not do great on red and green stuff. Test loss was > 600\n",
    "# When less 2 instead of 3 layers were allowed to train, the model did worse, having splotches of random color\n",
    "# And the colors were worse as well. Test loss was >600\n",
    "\n",
    "# Try 14\n",
    "# use model 9 with early stopping --> early stopping fixed so that best model is called after all training\n",
    "# 3 layers trainable\n",
    "# allows for \n",
    "\n",
    "# Using model OG, MSE test loss was 750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1382e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8049f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc810d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64200a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe710f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767768af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48e6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d0d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac69e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394f535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3139b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. torch.Size([32, 128, 64, 64])\n",
      "5. torch.Size([32, 128, 4, 4])\n",
      "9. torch.Size([32, 2, 2048, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64, 128, 128])) that is different to the input size (torch.Size([32, 2, 2048, 2048])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2048) must match the size of tensor b (128) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6q/lnztzmln25l88yjxl7j0pxth0000gn/T/ipykernel_3248/2110520241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mpreMod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0msampMod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreMod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampMod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fruit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/6q/lnztzmln25l88yjxl7j0pxth0000gn/T/ipykernel_3248/3779526903.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(color, trainLoader, valLoader, optimizer, epochs, album)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (128) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# class NewModel(nn.Module):\n",
    "#     def __init__(self, preModel):\n",
    "        \n",
    "#         super(NewModel, self).__init__()\n",
    "        \n",
    "#         # pre-model\n",
    "#         self.preModel = preModel\n",
    "\n",
    "#         ## Try 3 Architecture\n",
    "#         self.upsamp4 = nn.Sequential(nn.ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                                     nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "#         self.upsamp4.requires_grad = True\n",
    "#         self.upsamp5 = nn.Sequential(nn.ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                                     nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "#         self.upsamp5.requires_grad = True\n",
    "#         self.upsamp6 = nn.Sequential(nn.ConvTranspose2d(8, 8, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                                     nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "#         self.upsamp6.requires_grad = True\n",
    "#         self.upsamp7 = nn.Sequential(nn.ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                                     nn.BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "#         self.upsamp7.requires_grad = True\n",
    "#         self.upsamp8 = nn.Sequential(nn.ConvTranspose2d(4, 4, kernel_size=(2, 2), stride=(2, 2)),\n",
    "#                                     nn.BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "#         self.upsamp8.requires_grad = True\n",
    "#         self.upsamp9 = nn.Sequential(nn.ConvTranspose2d(4, 2, kernel_size=(2, 2), stride=(2, 2)))\n",
    "#         self.upsamp9.requires_grad = True\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = x.float()\n",
    "#         out = self.preModel.downsamp1(x)\n",
    "#         print(f\"1. {out.shape}\")\n",
    "#         out = self.preModel.downsamp2(out)\n",
    "#         out = self.preModel.downsamp3(out)\n",
    "#         out = self.preModel.downsamp4(out)\n",
    "#         out = self.preModel.downsamp5(out)\n",
    "#         print(f\"5. {out.shape}\")\n",
    "#         out = self.preModel.upsamp1(out)\n",
    "#         out = self.preModel.upsamp2(out)\n",
    "#         out = self.preModel.upsamp3(out)\n",
    "# #         out = self.upsamp4(out)\n",
    "# #         out = self.upsamp5(out)\n",
    "#         out = self.upsamp4(out)\n",
    "#         out = self.upsamp5(out)\n",
    "#         out = self.upsamp6(out)\n",
    "#         out = self.upsamp7(out)\n",
    "#         out = self.upsamp8(out)\n",
    "#         out = self.upsamp9(out)\n",
    "#         print(f\"9. {out.shape}\")\n",
    "        \n",
    "#         return out\n",
    "        \n",
    "#run color regressor\n",
    "lr = 0.01\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.Adam(cModel.parameters(), lr)\n",
    "path = \"./saved_models/color_architecture_9.pt\"\n",
    "preMod = colorizer()\n",
    "preMod.load_state_dict(torch.load(path))\n",
    "preMod.eval()\n",
    "sampMod = NewModel(preMod)\n",
    "trainModel(sampMod, food_train_loader, food_val_loader, optimizer, 90, 'fruit')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446654c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
