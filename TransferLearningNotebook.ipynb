{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26c2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sys import platform\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from colorizerII import colorizer\n",
    "from cnnColorizer import colorizer, trainModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d5f19",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45aa4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(folder):\n",
    "    files = glob.glob(folder)\n",
    "    data =[]\n",
    "    for f in files:\n",
    "        image = cv2.imread(f)\n",
    "        data.append(image)\n",
    "    return data\n",
    "\n",
    "\n",
    "def LoadLabInOrder(folder):\n",
    "\n",
    "    # find the largest number file to index (do not worry about how these next few lines work)\n",
    "    files = glob.glob(folder + \"*L.jpg\")\n",
    "    for i, f in enumerate(files):\n",
    "        f = f[f.rfind('/')+1:]\n",
    "        files[i] = f[0:f.rfind('.')-1]\n",
    "    maxFileNum = max([int(f) for f in files])\n",
    "    \n",
    "    # for each file index (e.g. ['0L.jpg', '0a.jpg', '0b.jpg'])\n",
    "    data = []\n",
    "    for i in range(0, maxFileNum):\n",
    "        # grab files in order 'a', 'b', 'L'\n",
    "        files = sorted(glob.glob(folder + str(i) + \"?.jpg\"), key=str.casefold)\n",
    "\n",
    "        # append each file\n",
    "        for f in files:\n",
    "            image = cv2.imread(f)\n",
    "\n",
    "            # only take one channel (all the channels here are the same)\n",
    "            data.append(image)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def group(data, album_length):\n",
    "    #group into chunks of three because of three sets of images in LAB color space\n",
    "    for i in range (0, album_length, 3):\n",
    "        yield data[i:i+3]\n",
    "  \n",
    "def make_numpy_format(image):\n",
    "    image = torch.swapaxes(image, 0, 1)\n",
    "    image = torch.swapaxes(image, 1, 2)\n",
    "    return image\n",
    "\n",
    "def make_tensor(image):\n",
    "    image = np.swapaxes(image, 2,1)\n",
    "    image = np.swapaxes(image, 1,0)\n",
    "    return image\n",
    "\n",
    "\n",
    "class imageDataset(Dataset):\n",
    "    def __init__(self,  l_color_space, ab_color_space,):\n",
    "        #dropping redundant channels\n",
    "        a = (ab_color_space[:, 0])\n",
    "        b = (ab_color_space[:, 1])\n",
    "        l = (l_color_space)\n",
    "        \n",
    "\n",
    "        #it seems that I will have to use permute \n",
    "        #to get from numpy image representation\n",
    "        #to torch tensor image \n",
    "        \n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.indices = len(l)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.indices\n",
    "      \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.a[index], self.b[index], self.l[index]    \n",
    "    \n",
    "    \n",
    "def convert_LAB(album):\n",
    "    converted_album = [[] for _ in range( len(album))]\n",
    "    \n",
    "    #code borrowed from https://towardsdatascience.com/computer-vision-101-working-with-color-images-in-python-7b57381a8a54\n",
    "    for index, image in enumerate(album):\n",
    "        #do LAB conversion here\n",
    "        image = np.asarray(image).astype('uint8')\n",
    "        converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        \n",
    "        names = []\n",
    "        channels = []\n",
    "        \n",
    "        for (name, chan) in zip((\"L\", \"a\", \"b\"), cv2.split(converted_image)):\n",
    "            names.append(name)\n",
    "            channels.append(chan)\n",
    "           # cv2.imshow(name, chan)\n",
    "           \n",
    "        converted_album[index] = list(zip(names, channels))\n",
    "              \n",
    "    return converted_album\n",
    "        \n",
    "def saveLAB(album, folder_name):\n",
    "    subfolder_dir = os.path.join(os.getcwd(), folder_name)\n",
    "    \n",
    "    if not os.path.exists(subfolder_dir):\n",
    "        os.mkdir(subfolder_dir)\n",
    "\n",
    "    count = 0\n",
    "    #each entry inside an LAB color space album is a tuple\n",
    "    for tup in album:\n",
    "       # image = image.cpu().detach().numpy()\n",
    "       names, channels = zip(*tup)\n",
    "       \n",
    "       for i in range(len(channels)):\n",
    "           name = names[i]\n",
    "           channel = channels[i]\n",
    "           cv2.imwrite(subfolder_dir + slash + str(count) + name + '.jpg', channel)\n",
    "           \n",
    "      \n",
    "       count +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d7bb2",
   "metadata": {},
   "source": [
    "### Get Original Images, Convert to Lab, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b6f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == 'darwin':\n",
    "    slash = '/'\n",
    "else: \n",
    "    slash = '\\\\'\n",
    "\n",
    "    \n",
    "home_dir = os.getcwd() \n",
    "\n",
    "#change this parameter depending on which album you want\n",
    "target_album = 'ColorfulLab'\n",
    "if target_album == 'LAB_TEST_FACES':\n",
    "    album = 'faces'\n",
    "else:\n",
    "    album = 'ColorfulLab'\n",
    "\n",
    "# if the specified directory does not exist, or if it exists but is empty\n",
    "if not os.path.exists(home_dir + slash + target_album) \\\n",
    "    or (os.path.exists(home_dir + slash + target_album) and not [name for name in os.listdir(\".\" + slash + target_album)]):\n",
    "\n",
    "    # get names of folders for colorful fruit data\n",
    "    foodFolders = [name for name in os.listdir(\".\" + slash + target_album)]\n",
    "\n",
    "    # get fruit images\n",
    "    food_images = []\n",
    "    for ff in foodFolders:\n",
    "        food_images.extend(load(home_dir + slash + target_album + slash + ff + slash + '*.jpg'))\n",
    "\n",
    "    # album_length = len(food_images)\n",
    "\n",
    "    for i, val in enumerate(food_images):\n",
    "        food_images[i] = cv2.resize(val, (128, 128))\n",
    "    food_images_lab = convert_LAB(food_images)\n",
    "    saveLAB(food_images_lab, \"ColorfulLab\")\n",
    "\n",
    "    # plot rgb image\n",
    "#     plt.imshow(cv2.cvtColor(food_images[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14621dea",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc5e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "Epochs = 100\n",
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "home_dir = os.getcwd() \n",
    "#change this parameter depending on which album you want\n",
    "target_album = 'ColorfulLab'\n",
    "if target_album == 'LAB_TEST_FACES':\n",
    "    album = 'faces'\n",
    "else:\n",
    "    album = 'ColorfulLab'\n",
    "\n",
    "\n",
    "food_data = LoadLabInOrder(home_dir + slash + target_album + slash)\n",
    "album_length = len(food_data)\n",
    "\n",
    "#group images into sets of 3   \n",
    "food_grouped_data = list(group(food_data, album_length))\n",
    "food_grouped_data = np.asarray(food_grouped_data)\n",
    "    \n",
    "#prepare grouped data for training and test\n",
    "food_train_images, food_test_images = train_test_split(food_grouped_data, test_size = 0.3)\n",
    "food_train_images, food_val_images = train_test_split(food_train_images, test_size = 0.1)\n",
    "\n",
    "#further separate them into X's and Y's where L is the input and AB are the targets (LAB colorspace)\n",
    "#remember the dimensions are Number of grouped images X Index of image\n",
    "#this needs to be flipped\n",
    "\n",
    "food_X_train = food_train_images[:, 2, :, :, 0]#.astype(dtype=object)\n",
    "food_y_train = food_train_images[:, 0:2, :, :, 0]#.astype(dtype=object)\n",
    "\n",
    "food_X_test = food_test_images[:, 2, :, :, 0]#.astype(dtype=object)\n",
    "food_y_test = food_test_images[:, 0:2, :, :, 0]#.astype(dtype=object)\n",
    "\n",
    "food_X_val = food_test_images[:, 2, :, :, 0]#.astype(dtype=object)\n",
    "food_y_val = food_test_images[:, 0:2, :, :, 0]#.astype(dtype=object)\n",
    "\n",
    "\n",
    "#prepare datasets for images\n",
    "food_train_dataset = imageDataset(food_X_train, food_y_train)\n",
    "food_test_dataset = imageDataset(food_X_test, food_y_test)\n",
    "food_val_dataset = imageDataset(food_X_val, food_y_val)\n",
    "\n",
    "# prepare dataloaders for batch training\n",
    "food_train_loader = torch.utils.data.DataLoader(dataset = food_train_dataset, batch_size = batch_size, shuffle=True)\n",
    "food_test_loader = torch.utils.data.DataLoader(dataset = food_test_dataset,  batch_size = batch_size, shuffle=True)\n",
    "food_val_loader = torch.utils.data.DataLoader(dataset = food_val_dataset,  batch_size = batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73b1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     for i, val in enumerate(batch):\n",
    "#         print(val[0].shape)\n",
    "#         print(val[1].shape)\n",
    "#         print(val[2].shape)\n",
    "            \n",
    "#     data = [item[0] for item in batch]\n",
    "#     target1 = [item[1] for item in batch]\n",
    "#     target2 = [item[2] for item in batch]\n",
    "#     target = torch.Tensor([target1, target2])\n",
    "#     return [data, target]\n",
    "\n",
    "\n",
    "# # #prepare dataloaders for batch training\n",
    "# food_train_loader = torch.utils.data.DataLoader(dataset = food_train_dataset, batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n",
    "# food_test_loader = torch.utils.data.DataLoader(dataset = food_test_dataset,  batch_size = batch_size, shuffle=True, collate_fn=my_collate)\n",
    "# food_val_loader = torch.utils.data.DataLoader(dataset = food_val_dataset,  batch_size = batch_size, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30dbf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(color, trainLoader, valLoader, optimizer, epochs, album):\n",
    "\n",
    "    #training loop: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "    #loss_values = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    val_ticker = 0\n",
    "    last_loss = 20000\n",
    "\n",
    "    # rows, cols = (2, Epochs)\n",
    "    # stored_images = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        color.train()\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        #I want batch to be of length 10 not 3 why?\n",
    "        for i, img in enumerate(trainLoader):\n",
    "            \n",
    "            a = img[0] # i changed these for clarity and less typing i didn't want to type batch everytime -hmk\n",
    "            b = img[1]\n",
    "            l = img[2]\n",
    "        \n",
    "            \n",
    "            #each batch is ten images so loop through all the images per batch\n",
    "            # no!!!! this defeats the point of batches if you loop through each image you've essentially made your batch size 1 -hmk\n",
    "            \n",
    "            # for index, images in enumerate(batch):\n",
    "                # get the inputs; data is a list of tensors [chrominance_a_tensor, chrominance_b_tensor, grayscale_l_tensor]\n",
    "                # different images!\n",
    "        \n",
    "        \n",
    "            #labels = torch.tensor((label_a, label_b))\n",
    "            #might not be necessary to drop duplicates\n",
    "            labels = torch.stack((a, b), 1).float().to(device)\n",
    "            input_l = torch.unsqueeze(l, 1).to(device)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = color((input_l))\n",
    "            # outputs = outputs.view(2, size)\n",
    "            \n",
    "            #flatten labels along dimension 0\n",
    "            labels = torch.flatten(labels, 0, 1)\n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "                    \n",
    "        train_loss.append(loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                color.eval()\n",
    "                for data in valLoader:\n",
    "                    val_l = torch.unsqueeze(data[2], 1).to(device)\n",
    "                    val_outputs = color(val_l)\n",
    "                    val_labels = torch.stack((data[0], data[1]), 1).float().to(device)\n",
    "                    val_loss = criterion(val_outputs, torch.flatten(val_labels, 0, 1))\n",
    "                    running_val_loss += val_loss\n",
    "\n",
    "            validation_loss.append(running_val_loss)\n",
    "            print(\"\\nNumber Of Images Tested =\", len(valLoader)*batch_size)\n",
    "            print(\"Validation MSE Loss =\", (running_val_loss/len(valLoader)))\n",
    "\n",
    "            if (running_val_loss/len(valLoader)) - last_loss >= 0.1:\n",
    "                path = f\"./chkpt_{album}/color_model_{epoch}.pt\"\n",
    "                torch.save(color.state_dict(), path)\n",
    "            last_loss = (running_val_loss/len(valLoader))\n",
    "\n",
    "            # once done with a loop I want to print out the target image \n",
    "            # # and colorized image for comparison    \n",
    "            # sample_target = cv2.merge([l[0].detach().numpy(), a[0].detach().numpy(), b[0].detach().numpy()]) \n",
    "            # sample_target = cv2.cvtColor(sample_target, cv2.COLOR_LAB2RGB)\n",
    "            #plt.imshow(sample_target)\n",
    "            \n",
    "            sample_target = cv2.merge([l[0].cpu().detach().numpy(), a[0].cpu().detach().numpy(), b[0].cpu().detach().numpy()]) \n",
    "            sample_target = cv2.cvtColor(sample_target, cv2.COLOR_LAB2RGB)\n",
    "            #plt.imshow(sample_target)\n",
    "        \n",
    "            colorized_a = outputs[0].cpu().detach().numpy().astype(np.uint8)\n",
    "            colorized_b = outputs[1].cpu().detach().numpy().astype(np.uint8)\n",
    "            sample_colorized = cv2.merge([l[0].detach().numpy(), colorized_a, colorized_b])\n",
    "            sample_colorized = cv2.cvtColor(sample_colorized, cv2.COLOR_LAB2RGB)\n",
    "            #plt.imshow(sample_colorized)                   dont need these anymore bc im just saving the images as pngs instead -hmk\n",
    "            # stored_images[0][epoch] = sample_target\n",
    "            # stored_images[1][epoch] = sample_colorized\n",
    "            cv2.imwrite(f\"./chkpt_{album}/images/target_image_{epoch}.png\",sample_target)\n",
    "            cv2.imwrite(f\"./chkpt_{album}/images/output_image_{epoch}.png\",sample_colorized) # -hmk\n",
    "\n",
    "        print('Epoch {} of {}, Training MSE Loss: {:.3f}'.format( epoch+1, epochs, running_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e824a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnColorizer import colorizer\n",
    "cModel = torch.load('./saved_models/model_architecture_11.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4e9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Of Images Tested = 224\n",
      "Validation MSE Loss = tensor(517.5297)\n",
      "Epoch 1 of 10, Training MSE Loss: 492.309\n",
      "Epoch 2 of 10, Training MSE Loss: 407.637\n",
      "Epoch 3 of 10, Training MSE Loss: 356.486\n",
      "Epoch 4 of 10, Training MSE Loss: 335.115\n",
      "Epoch 5 of 10, Training MSE Loss: 311.796\n",
      "Epoch 6 of 10, Training MSE Loss: 308.847\n",
      "Epoch 7 of 10, Training MSE Loss: 296.485\n",
      "Epoch 8 of 10, Training MSE Loss: 291.133\n",
      "Epoch 9 of 10, Training MSE Loss: 296.058\n",
      "Epoch 10 of 10, Training MSE Loss: 279.320\n"
     ]
    }
   ],
   "source": [
    "#run color regressor\n",
    "lr = 0.01\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.Adam(cModel.parameters(), lr)\n",
    "trainModel(cModel, food_train_loader, food_val_loader, optimizer, 90, 'fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4077b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Of Images Tested = 224\n",
      "Testing MSE Loss = tensor(329.9818)\n"
     ]
    }
   ],
   "source": [
    "running_test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    cModel.eval()\n",
    "    for data in food_test_loader:\n",
    "        test_l = torch.unsqueeze(data[2], 1).to(device)\n",
    "        test_outputs = cModel(test_l)\n",
    "        test_labels = torch.stack((data[0], data[1]), 1).float().to(device)\n",
    "        test_loss = criterion(test_outputs, torch.flatten(test_labels, 0, 1))\n",
    "        running_test_loss += test_loss\n",
    "\n",
    "print(\"\\nNumber Of Images Tested =\", len(food_test_loader)*batch_size)\n",
    "print(\"Testing MSE Loss =\", (running_test_loss/len(food_test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f195b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37926ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cModel.eval()\n",
    "    for data in food_test_loader:\n",
    "        test_l = torch.unsqueeze(data[2], 1).to(device)\n",
    "        test_outputs = cModel(test_l)\n",
    "        test_a = torch.unsqueeze(data[0], 1).to(device)\n",
    "        test_b = torch.unsqueeze(data[1], 1).to(device)\n",
    "        \n",
    "#         test_labels = torch.stack((data[0], data[1]), 1).float().to(device)\n",
    "#         test_loss = criterion(test_outputs, torch.flatten(test_labels, 0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d06aede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = torch.stack((torch.squeeze(test_l), torch.squeeze(test_a), torch.squeeze(test_b)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c54f3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6122035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3f7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134087a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f5e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
